{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6561f154-aec2-4366-a182-7c3c66140744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "99413d53-c512-4859-8c74-27b9708d162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y=np.array([ [0,0,0,1] ]).T"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2428db75-e468-43dd-8be5-b273a2e1d071",
   "metadata": {},
   "source": [
    "Keep Y as 2D array also i.e (4*1)\n",
    "Method 2.\n",
    "## Y=np.array([[0],[0],[0],[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "63393c6e-69f4-44d4-86ee-cadebdefc647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 2), (4, 1))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,Y.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e022ce39-454d-4137-8408-335a533f0d77",
   "metadata": {},
   "source": [
    "For now We want to make as basic as Logistoc Regression With No hidden Layer ..Only one unit in Output layer \n",
    ".. Activation function will sigmoid "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b76d0f-4094-455e-9c57-855b20610a3d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Sigmoid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "162109e1-173e-4ea3-8e06-fbb731a2e248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e : 2.718281828459045\n",
      "[0.73105858 0.88079708 0.95257413]\n"
     ]
    }
   ],
   "source": [
    "def sig(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "    \n",
    "print('e :',np.exp(1)) \n",
    "\n",
    "z=np.array([1.0,2.0,3.0])\n",
    "print(sig(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c4e3173-5584-4d41-809b-5c28fb4c5eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivativeSig(z):\n",
    "    return sig(z)*(1-sig(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234341c7-8617-4c12-a30a-668383cead6b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## No Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d457cea-669d-4ad4-a041-0280284fb4bf",
   "metadata": {},
   "source": [
    "### Initialise weights"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7436af1-0a72-412b-8c14-e00b2981cdf3",
   "metadata": {},
   "source": [
    "> Pass the shape as input in np.random.random((shape)) # (2,1) in this Case\n",
    "\n",
    "> Random Values will be between 0-1\n",
    "> Note np.random.random gives array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "88507276-4826-4306-aed7-2a3f9989e426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.54264129],\n",
       "        [-0.9584961 ]]),\n",
       " array([[0.26729647]]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "weights=2*np.random.random((2,1))-1\n",
    "bias=2*np.random.random((1,1))-1\n",
    "weights,bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93036c27-094c-464a-80dc-3197f010f4df",
   "metadata": {},
   "source": [
    "### Forward Propagation Without any Hidden Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0dd8032-510b-4d96-80be-45597d4a143e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56642907],\n",
       "       [0.33376626],\n",
       "       [0.69209624],\n",
       "       [0.46292857]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output0 = X\n",
    "output = sig(np.dot(output0,weights)+bias)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f187ba0-c213-4c65-8a14-5397005de0f9",
   "metadata": {},
   "source": [
    "### BackPropagation With out Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0479a957-4745-4bff-83e8-ba8df06878d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_term = output-Y\n",
    "input_j = np.dot(output0,weights)+ bias\n",
    "second_term = derivativeSig(input_j)\n",
    "first_two = first_term*second_term\n",
    "first_two.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "728163aa-782e-4fa0-8e5d-0ffaa7ac50df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.54250173],\n",
       "        [-0.95790299]]),\n",
       " (2, 1))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_changes=np.array([[0.0],[0.0]])\n",
    "for i in range(2):\n",
    "    for j in range(4):\n",
    "        w_changes[i,0]+=first_two[j,0]*output0[j,i]\n",
    "        # Why 0 as shape is 4*1 so onlu one column.possible\n",
    "learningRate = 0.01\n",
    "weights = weights - learningRate*w_changes\n",
    "weights , weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9cab408f-d22d-4ad6-b518-25e312221436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.26502366]]), (1, 1))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_changes=np.array([[0.0]])\n",
    "\n",
    "for j in range(4):\n",
    "    bias_changes[0,0]+=first_two[j,0]\n",
    "        # Why 0 as shape is 4*1 so onlu one column.possible\n",
    "\n",
    "bias = bias - learningRate*bias_changes\n",
    "bias , bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a19bd0-0366-47fe-bd21-af0b56eefd31",
   "metadata": {},
   "source": [
    "### Complete Neural Netword"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8e63686-876b-4352-a7ef-abb23a9fa37e",
   "metadata": {},
   "source": [
    ">> Complied Code -I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bedb7b91-a323-4b0e-806e-ef15dc4a0379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 953 ms\n",
      "Wall time: 992 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[5.47310337],\n",
       "        [5.47310335]]),\n",
       " array([[-8.30237529]]),\n",
       " array([[2.47865776e-04],\n",
       "        [5.57627201e-02],\n",
       "        [5.57627213e-02],\n",
       "        [9.33629774e-01]]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y=np.array([ [0,0,0,1] ]).T\n",
    "\n",
    "## Inialise Weights \n",
    "np.random.seed(10)\n",
    "weights=2*np.random.random((2,1))-1\n",
    "bias=2*np.random.random((1,1))-1\n",
    "\n",
    "learningRate = 0.1\n",
    "output0 = X\n",
    "\n",
    "for iter in range(10000):\n",
    "    # print(iter)\n",
    "    output = sig(np.dot(output0,weights)+bias)\n",
    "    \n",
    "    first_term = output-Y\n",
    "    \n",
    "    input_j = np.dot(output0,weights)+ bias\n",
    "    second_term = derivativeSig(input_j)\n",
    "    \n",
    "    first_two = first_term*second_term\n",
    "     \n",
    "    ## Update Weights\n",
    "    w_changes=np.array([[0.0],[0.0]])\n",
    "    for i in range(2):\n",
    "        for j in range(4):\n",
    "            w_changes[i][0]+=first_two[j][0]*output0[j][i]\n",
    "    \n",
    "    weights = weights - learningRate*w_changes\n",
    "\n",
    "    ## Update Bias\n",
    "    bias_changes=np.array([[0.0]])\n",
    "    for j in range(4):\n",
    "        bias_changes[0][0]+=first_two[j][0]\n",
    "            \n",
    "    bias = bias - learningRate*bias_changes\n",
    "    \n",
    "output = sig(np.dot(X,weights)+bias)\n",
    "weights,bias,output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c3d4d057-abcf-4261-a3b3-59f44f28e954",
   "metadata": {},
   "source": [
    "So our Output is now much closer to  [0,0,0,1] that's good.. This was acheived keep little high value of Learning rate and iteration"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ffe2436d-e944-4b34-b750-e47b15b511f7",
   "metadata": {},
   "source": [
    ">> Complied Code -II\n",
    "# With Vector dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56bdb05d-590b-4da4-9585-0920924da316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 875 ms\n",
      "Wall time: 898 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[5.47310337],\n",
       "        [5.47310335]]),\n",
       " array([[-8.30237529]]),\n",
       " array([[2.47865776e-04],\n",
       "        [5.57627201e-02],\n",
       "        [5.57627213e-02],\n",
       "        [9.33629774e-01]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y=np.array([ [0,0,0,1] ]).T\n",
    "\n",
    "## Inialise Weights \n",
    "np.random.seed(10)\n",
    "weights=2*np.random.random((2,1))-1\n",
    "bias=2*np.random.random((1,1))-1\n",
    "\n",
    "learningRate = 0.1\n",
    "output0 = X\n",
    "\n",
    "for iter in range(10000):\n",
    "    # print(iter)\n",
    "    inputO = np.dot(output0,weights)+bias\n",
    "    outputO = sig(inputO)\n",
    "    \n",
    "    first_term = outputO-Y\n",
    "    \n",
    "    input_j = np.dot(output0,weights)+ bias\n",
    "    second_term = derivativeSig(input_j)\n",
    "    \n",
    "    first_two = first_term*second_term\n",
    "    ## Update Weights\n",
    "    w_changes=np.dot(output0.T,first_two)\n",
    "    weights = weights - learningRate*w_changes\n",
    "    \n",
    "    ## Update Bias\n",
    "    bias_changes=np.sum(first_two,axis=0)\n",
    "    bias = bias - learningRate*bias_changes\n",
    "\n",
    "inputO = np.dot(output0,weights)+bias\n",
    "outputO = sig(inputO)\n",
    "weights,bias,outputO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd31eb0-e1d5-4eaf-b05d-4069955cd07e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## With Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "daae7726-1544-4424-8ae9-6b25c3b1ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y=np.array([[1,0,0,1]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826e4150-fe72-4fed-9558-5a4fcd318a49",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Initialise weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "89615cb4-df4a-44f4-bd73-7c991b41e8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.25091976,  0.90142861],\n",
       "        [ 0.46398788,  0.19731697]]),\n",
       " array([[-0.68796272, -0.68801096]]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "weightsH=2*np.random.random((2,2))-1\n",
    "biasH=2*np.random.random((1,2))-1\n",
    "weightsH,biasH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6e2b5b51-5514-49dc-912a-36317b1c77b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.18462594],\n",
       "        [-0.88926792]]),\n",
       " array([[0.57706975]]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(40)\n",
    "weightsO=2*np.random.random((2,1))-1\n",
    "biasO=2*np.random.random((1,1))-1\n",
    "weightsO,biasO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7bea6a-8bed-4380-b12b-e2bdc3368a93",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Forward Propagation With Hidden Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d5b8d633-b2bb-4fee-ae3f-83b91923cdfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.33448643, 0.33447569],\n",
       "        [0.4442392 , 0.37973009],\n",
       "        [0.28112613, 0.55315282],\n",
       "        [0.38345841, 0.60126401]]),\n",
       " array([[0.55425464],\n",
       "        [0.53926148],\n",
       "        [0.50831561],\n",
       "        [0.49289762]]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output0 = X\n",
    "inputH = np.dot(output0,weightsH)+biasH\n",
    "outputH = sig(inputH)\n",
    "inputO = np.dot(outputH,weightsO)+biasO\n",
    "outputO = sig(inputO)\n",
    "outputH,outputO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8f253d-71fe-4a8a-9f4e-08462df82389",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### BackPropagation With Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fea1c750-05b7-4a34-a82b-981d107716ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_term_OutputL = outputO-Y\n",
    "second_term_OutputL = derivativeSig(inputO)\n",
    "first_two_OutputL = first_term_OutputL * second_term_OutputL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c39b7b87-2a35-4780-9ebc-cb266f5df7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_term_HiddenL = np.dot(first_two_OutputL,weightsO.T)\n",
    "second_term_HiddenL = derivativeSig(inputH)\n",
    "first_two_HiddenL = first_term_HiddenL * second_term_HiddenL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e29ed58-aa18-4686-a1da-3c20445880f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "changes_OutputL_weights = np.dot(outputH.T,first_two_OutputL)\n",
    "changes_OutputL_bias = np.sum(first_two_OutputL,axis=0,keepdims=True) ## will give vector rather than single number \n",
    "\n",
    "changes_HiddenL_weights = np.dot(outputO.T,first_two_HiddenL)\n",
    "changes_HiddenL_bias = np.sum(first_two_HiddenL,axis=0,keepdims=True) ## will give vector rather than single number \n",
    "\n",
    "lr = 0.01\n",
    "weightsO = weightsO -lr*changes_OutputL_weights\n",
    "biasO = biasO -lr*changes_OutputL_bias\n",
    "\n",
    "weightsH = weightsH -lr*changes_HiddenL_weights\n",
    "biasH = biasH -lr*changes_HiddenL_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d38b0ef-6749-4036-8881-a7051944f0f9",
   "metadata": {},
   "source": [
    "### Complete General Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c47a9b-c7d6-4da2-84eb-6626e0b171fb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### NN-I "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10bc67ca-6645-440c-aaeb-3050e12cea09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.06163868],\n",
       "        [0.94418828],\n",
       "        [0.94420396],\n",
       "        [0.05927355]]),\n",
       " array([[3.84775806, 5.51867251],\n",
       "        [3.84655814, 5.5141638 ]]),\n",
       " array([[-7.90825684],\n",
       "        [ 7.37465969]]),\n",
       " array([[-5.90851032, -2.31351363]]),\n",
       " array([[-3.36521572]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y=np.array([[0,1,1,0]]).T\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "weightsH = 2*np.random.random((2,2))-1\n",
    "biasH = 2*np.random.random((1,2))-1\n",
    "\n",
    "weightsO = 2*np.random.random((2,1))-1\n",
    "biasO = 2*np.random.random((1,1))-1\n",
    "\n",
    "lr = 0.1\n",
    "output0 = X\n",
    "\n",
    "for iter in range(10000):\n",
    "    \n",
    "    inputH = np.dot(output0,weightsH)+biasH\n",
    "    outputH = sig(inputH)\n",
    "    inputO = np.dot(outputH,weightsO)+biasO\n",
    "    outputO = sig(inputO)\n",
    "    \n",
    "    first_term_OutputL = outputO-Y\n",
    "    second_term_OutputL = derivativeSig(inputO)\n",
    "    first_two_OutputL = first_term_OutputL * second_term_OutputL\n",
    "    \n",
    "    first_term_HiddenL = np.dot(first_two_OutputL,weightsO.T)\n",
    "    second_term_HiddenL = derivativeSig(inputH)\n",
    "    first_two_HiddenL = first_term_HiddenL * second_term_HiddenL\n",
    "    \n",
    "    changes_OutputL_weights = np.dot(outputH.T,first_two_OutputL)\n",
    "    changes_OutputL_bias = np.sum(first_two_OutputL,axis=0,keepdims=True) ## will give vector rather than single number \n",
    "    \n",
    "    changes_HiddenL_weights = np.dot(output0.T,first_two_HiddenL)\n",
    "    changes_HiddenL_bias = np.sum(first_two_HiddenL,axis=0,keepdims=True) ## will give vector rather than single number \n",
    "    \n",
    "    weightsO = weightsO -lr*changes_OutputL_weights\n",
    "    biasO = biasO -lr*changes_OutputL_bias\n",
    "    \n",
    "    weightsH = weightsH -lr*changes_HiddenL_weights\n",
    "    biasH = biasH -lr*changes_HiddenL_bias\n",
    "\n",
    "\n",
    "inputH = np.dot(output0,weightsH)+biasH\n",
    "outputH = sig(inputH)\n",
    "inputO = np.dot(outputH,weightsO)+biasO\n",
    "outputO = sig(inputO)\n",
    "\n",
    "outputO,weightsH,weightsO,biasH,biasO "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6af17bc-b63d-48e2-b7d4-1aa3adb38e01",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### NN-II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a81b22ee-3606-4efb-97f9-eaa35c2a9423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.04185399],\n",
       "        [0.95113438],\n",
       "        [0.95441066],\n",
       "        [0.04800054]]),\n",
       " array([[ 4.5109361 ,  5.66190921, -1.13713739,  0.85868769],\n",
       "        [-5.12642605, -5.31381498, -1.17077569,  0.97193533]]),\n",
       " array([[ 7.70407925],\n",
       "        [-7.33482918],\n",
       "        [-0.92231126],\n",
       "        [ 1.57701299]]),\n",
       " array([[-2.37537617,  2.78731873, -0.95504034,  1.03731628]]),\n",
       " array([[2.21504019]]))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y=np.array([[0,1,1,0]]).T\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "weightsH = 2*np.random.random((2,4))-1\n",
    "biasH = 2*np.random.random((1,4))-1\n",
    "\n",
    "weightsO = 2*np.random.random((4,1))-1\n",
    "biasO = 2*np.random.random((1,1))-1\n",
    "\n",
    "lr = 0.1\n",
    "output0 = X\n",
    "\n",
    "for iter in range(10000):\n",
    "    \n",
    "    inputH = np.dot(output0,weightsH)+biasH\n",
    "    outputH = sig(inputH)\n",
    "    inputO = np.dot(outputH,weightsO)+biasO\n",
    "    outputO = sig(inputO)\n",
    "    \n",
    "    first_term_OutputL = outputO-Y\n",
    "    second_term_OutputL = derivativeSig(inputO)\n",
    "    first_two_OutputL = first_term_OutputL * second_term_OutputL\n",
    "    \n",
    "    changes_OutputL_weights = np.dot(outputH.T,first_two_OutputL)\n",
    "    changes_OutputL_bias = np.sum(first_two_OutputL,axis=0,keepdims=True) ## will give vector rather than single number \n",
    "    \n",
    "    weightsO = weightsO -lr*changes_OutputL_weights\n",
    "    biasO = biasO -lr*changes_OutputL_bias\n",
    "    \n",
    "    first_term_HiddenL = np.dot(first_two_OutputL,weightsO.T)\n",
    "    second_term_HiddenL = derivativeSig(inputH)\n",
    "    first_two_HiddenL = first_term_HiddenL * second_term_HiddenL\n",
    "    \n",
    "    changes_HiddenL_weights = np.dot(output0.T,first_two_HiddenL)\n",
    "    changes_HiddenL_bias = np.sum(first_two_HiddenL,axis=0,keepdims=True) ## will give vector rather than single number \n",
    "    \n",
    "    weightsH = weightsH -lr*changes_HiddenL_weights\n",
    "    biasH = biasH -lr*changes_HiddenL_bias\n",
    "\n",
    "\n",
    "inputH = np.dot(output0,weightsH)+biasH\n",
    "outputH = sig(inputH)\n",
    "inputO = np.dot(outputH,weightsO)+biasO\n",
    "outputO = sig(inputO)\n",
    "\n",
    "outputO,weightsH,weightsO,biasH,biasO "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067d56a6-288f-44d1-b87e-eb142c16bc76",
   "metadata": {},
   "source": [
    "#### Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0369882-f73c-427f-b1fc-49fa3e3ccf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHiddenWeightsBias(num_hidden , n_feature ):\n",
    "    np.random.seed(10)\n",
    "    HLW = np.random.random((n_feature,num_hidden[0]))\n",
    "    HLB = np.random.random((1,num_hidden[0]))\n",
    "    List1 =[HLW]\n",
    "    List2 =[HLB]\n",
    "    n = len(num_hidden)\n",
    "    j=1\n",
    "    while(j<n):\n",
    "        HLW = np.random.random((num_hidden[j-1],num_hidden[j]))\n",
    "        HLB = np.random.random((1,num_hidden[j]))\n",
    "        List1.append(HLW)\n",
    "        List2.append(HLB)\n",
    "        j+=1\n",
    "    return List1,List2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1b27f41-a1c2-4fc1-86f3-62a77581dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def farwardProagation(X,weights,bias):\n",
    "    LayerOutputs = [X]\n",
    "    LayerInputs = [X]\n",
    "    \n",
    "    j=0\n",
    "    n = len(weights)\n",
    "    while j<n:\n",
    "        input = np.dot(LayerOutputs[-1],weights[j])+bias[j]\n",
    "        output = sig(input)\n",
    "        LayerOutputs.append(output)\n",
    "        LayerInputs.append(input)\n",
    "        j+=1\n",
    "        \n",
    "    return LayerOutputs ,LayerInputs  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feef428e-544a-4ce2-ac82-8b1f6e0ec1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardPropagation1(Y,weights,bias,outputs,inputs,lr):\n",
    "\n",
    "    first_term = outputs[-1]-Y  # First Term of Output Layer\n",
    "    \n",
    "    second_term = derivativeSig(inputs[-1])\n",
    "    first_two = first_term * second_term\n",
    "\n",
    "    changes_OutputL_weights = np.dot(outputs[-2].T , first_two)\n",
    "    changes_OutputL_bias = np.sum(first_two ,axis=0 ,keepdims=True) ## will give vector rather than single number \n",
    "    \n",
    "    weights[-1] = weights[-1] -lr*changes_OutputL_weights\n",
    "    bias[-1] = bias[-1] - lr * changes_OutputL_bias\n",
    "    \n",
    "    j= len(weights)-1\n",
    "    while j>0:\n",
    "        first_term = np.dot( first_two ,weights[j].T )\n",
    "        \n",
    "        second_term = derivativeSig( inputs[j] )\n",
    "        first_two = first_term * second_term\n",
    "        \n",
    "        changes_HiddenL_weights = np.dot(outputs[j-1].T ,first_two)\n",
    "        changes_HiddenL_bias = np.sum(first_two ,axis = 0 ,keepdims = True) ## will give vector rather than single number \n",
    "        \n",
    "        weights[j-1] = weights[j-1] -lr*changes_HiddenL_weights\n",
    "        bias[j-1] = bias[j-1] -lr*changes_HiddenL_bias\n",
    "\n",
    "        j-=1\n",
    "    return weights,bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a6bf499-e446-4a4e-8c3b-8d049417f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardPropagation2(Y,weights,bias,outputs,inputs,lr):\n",
    "\n",
    "    first_term = outputs[-1]-Y  # First Term of Output Layer\n",
    "    \n",
    "    j= len(weights)-1\n",
    "    while j>=0:\n",
    "        \n",
    "        second_term = derivativeSig( inputs[j+1] )\n",
    "        first_two = first_term * second_term\n",
    "        \n",
    "        changes_HiddenL_weights = np.dot(outputs[j].T ,first_two)\n",
    "        changes_HiddenL_bias = np.sum(first_two ,axis = 0 ,keepdims = True) ## will give vector rather than single number \n",
    "        \n",
    "        weights[j] = weights[j] -lr*changes_HiddenL_weights\n",
    "        bias[j] = bias[j] -lr*changes_HiddenL_bias\n",
    "\n",
    "        if(j>0):\n",
    "            first_term = np.dot( first_two ,weights[j].T ) \n",
    "            ## just find first_Term for previpus layer  from first_two of this layer\n",
    "        j-=1\n",
    "    return weights,bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d83e6f-248b-40e3-afc9-7656400fd4ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### NN-III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "f4c35e59-09a5-4a1e-b748-e52b5a15b112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Weights and biases : 3\n",
      "Shape of Outputs and Inputs : 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00749631],\n",
       "       [0.99327712],\n",
       "       [0.9938576 ],\n",
       "       [0.00720713]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y=np.array([[0,1,1,0]]).T\n",
    "lr = 1\n",
    "\n",
    "num_hidden=[5,4,]\n",
    "n_output = 1\n",
    "\n",
    "num_hidden += [n_output]\n",
    "n_feature = X.shape[1]\n",
    "\n",
    "weights ,bias = getHiddenWeightsBias(num_hidden,n_feature) \n",
    "\n",
    "print(\"Shape of Weights and biases :\",len(weights))\n",
    "print(\"Shape of Outputs and Inputs :\",len(outputs))\n",
    "## output size is +1 than weights as it contains X at output[0] and input[0]\n",
    "\n",
    "for iter in range(10000):\n",
    "    outputs,inputs = farwardProagation(X,weights,bias)\n",
    "    weights,bias  = backwardPropagation2(Y,weights,bias,outputs,inputs,lr)\n",
    "\n",
    "\n",
    "weights ,bias\n",
    "outputs,inputs\n",
    "outputs[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0cc183-2842-41fa-ae97-5fbf252ddba4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### NN-IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "9401e176-1008-4ec8-966f-e03879da7a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2) (4, 3) \n",
      "\n",
      "Shape of Weights and biases : 3\n",
      "Shape of Outputs and Inputs : 4\n",
      "\n",
      "(2, 5) (1, 5)\n",
      "(5, 4) (1, 4)\n",
      "(4, 1) (1, 1)\n",
      "\n",
      "(2, 5) (1, 5)\n",
      "(5, 4) (1, 4)\n",
      "(4, 3) (1, 3)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7.23388120e-03, 9.92668645e-01, 9.99348796e-01],\n",
       "       [9.91649846e-01, 8.19296837e-03, 1.07604746e-02],\n",
       "       [9.90457006e-01, 9.65096403e-03, 9.89549616e-01],\n",
       "       [1.04269856e-02, 9.89615019e-01, 6.18970565e-04]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y=np.array([[0,1,1,0],[1,0,0,1],[1,0,1,0]]).T\n",
    "print(X.shape,Y.shape,'\\n')\n",
    "lr = 1\n",
    "\n",
    "num_hidden=[5,4,]\n",
    "n_output = 1\n",
    "\n",
    "num_hidden += [n_output]\n",
    "n_feature = X.shape[1]\n",
    "\n",
    "weights ,bias = getHiddenWeightsBias(num_hidden,n_feature) \n",
    "\n",
    "print(\"Shape of Weights and biases :\",len(weights))\n",
    "print(\"Shape of Outputs and Inputs :\",len(outputs))\n",
    "print()\n",
    "## output size is +1 than weights as it contains X at output[0] and input[0]\n",
    "\n",
    "for i in range(len(bias)):\n",
    "    print(weights[i].shape,bias[i].shape)\n",
    "print()\n",
    "\n",
    "for iter in range(10000):\n",
    "    \n",
    "    outputs,inputs = farwardProagation(X,weights,bias)\n",
    "    weights,bias  = backwardPropagation2(Y,weights,bias,outputs,inputs,lr)\n",
    "\n",
    "for i in range(len(bias)):\n",
    "    print(weights[i].shape,bias[i].shape)\n",
    "print()\n",
    "\n",
    "weights ,bias\n",
    "outputs,inputs\n",
    "outputs[-1]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad023caf-d207-4ead-9e2e-a13f3ff63b89",
   "metadata": {},
   "source": [
    "Note :: Even Though We will be getting Better Output by Keeping n_output = 1 for number of outputs = 3 \n",
    "[i.e is due to broadcasting feature in  numpy arrays]..\n",
    "But broadcasting wont be working as good as with n_output = number of outputs .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf99daaf-e3a2-4114-8693-9106d5789f37",
   "metadata": {},
   "source": [
    "#### NN-V : Neural Network Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "658de4b1-0955-4109-a4aa-9e849f0a2f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,weights , bias):\n",
    "\n",
    "    output = X\n",
    "    for w,b in zip(weights,bias):\n",
    "        input = np.dot(output,w) +b\n",
    "        output = sig(input)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "1bab63fc-2590-4ef3-a79d-630300670365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuralNetwork(X,Y,num_hidden,lr,iterations):\n",
    "\n",
    "    n_output = Y.shape[1]\n",
    "    n_feature = X.shape[1]\n",
    "    \n",
    "    num_hidden += [n_output]\n",
    "    \n",
    "    weights ,bias = getHiddenWeightsBias(num_hidden,n_feature) \n",
    "   \n",
    "    for iter in range(iterations):\n",
    "        outputs,inputs = farwardProagation(X,weights,bias)\n",
    "        weights,bias  = backwardPropagation2(Y,weights,bias,outputs,inputs,lr)\n",
    "        \n",
    "    return weights,bias\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4064cc82-a4a9-4441-b303-e8fd575db1c9",
   "metadata": {},
   "source": [
    "## So using Above Functionns We can Use our Neural With any number of  hidden layer with , any number of units in it ..and \n",
    "AlSo Could also work well on  linearly seperable problems very welll. Without any Hidden Layer as Well\n",
    "i.e as Logisitic Regression ..\n",
    "i.e Note Calcuation for dE/dW will be diffenerent as E is different For The way we are implementing in NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "ea2d472e-c296-4471-b9d9-18535f8cad4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.45452563e-04, 9.99752182e-01],\n",
       "       [5.55922322e-02, 9.44240658e-01],\n",
       "       [5.55922323e-02, 9.44240657e-01],\n",
       "       [9.33834020e-01, 6.63661785e-02]])"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y=np.array([[0,1,1,0],[1,0,0,1],[1,0,1,0]]).T\n",
    "Y_=np.array([[0,0,0,1],[1,1,1,0]]).T\n",
    "\n",
    "num_hidden= []\n",
    "lr = 0.1\n",
    "iters = 10000\n",
    "weights,bias = NeuralNetwork(X,Y_,num_hidden ,lr , iters)\n",
    "Y_pred = predict(X,weights,bias)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc39428-6873-4298-8106-643c0f5701cc",
   "metadata": {},
   "source": [
    "### Iris Dataset "
   ]
  },
  {
   "cell_type": "raw",
   "id": "666aae0a-b55f-45a9-86bb-ce289b0fd33e",
   "metadata": {},
   "source": [
    "Function for Transforming y from shape (n,) to n,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "f801af34-0762-464c-885b-eaec7d0fc599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transform(Y):\n",
    "    uniq = np.unique(Y)\n",
    "    n = len(uniq)\n",
    "    \n",
    "    label_to_index = {}\n",
    "    index_to_label = {}\n",
    "    \n",
    "    index = 0 \n",
    "    while index < n:\n",
    "        label = uniq[index]\n",
    "        \n",
    "        label_to_index[label] = index\n",
    "        index_to_label[index ] = label\n",
    "        index += 1\n",
    "\n",
    "    Arr = []\n",
    "    for label in  Y:\n",
    "        vector = np.zeros(n)\n",
    "        index = label_to_index[label]\n",
    "        vector[index] = 1\n",
    "        Arr.append(vector)\n",
    "        \n",
    "    return np.array(Arr),index_to_label  \n",
    "    \n",
    "def InverseTransform(Y,index_to_label):\n",
    "\n",
    "    Arr = []    \n",
    "    for vector in  Y:\n",
    "        index = np.argmax(vector)\n",
    "        label = index_to_label[index]\n",
    "        Arr.append(label)\n",
    "\n",
    "    return np.array(Arr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "8514734d-ddcc-426e-b6b7-9c50fbf6054e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "np.shape(X) , np.shape(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "238a575c-a414-4cd0-9bc9-f11751d96ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train ,X_test , Y_train,Y_test = train_test_split(X ,Y ,stratify=Y ,random_state = 42,shuffle=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "01936b9c-e0fc-406e-bf5c-41cc692abe15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "Y_train_t,index_to_label =  Transform(Y_train)\n",
    "print(Y_train_t[70:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "id": "2628a14f-3176-4bcb-bb19-865c985f54be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.11 s\n",
      "Wall time: 3.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "lr = 0.005\n",
    "num_hidden=[2,3]\n",
    "iterations=10000\n",
    "# lr = 0.005\n",
    "# num_hidden=[]\n",
    "# iterations=10000\n",
    "\n",
    "weights,bias = NeuralNetwork(X_train,Y_train_t,num_hidden,lr,iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "id": "dc8d663b-7ce8-40cd-9c4b-fc5c209adb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.93377348e-03, 8.95621271e-01, 2.79828656e-02],\n",
       "       [2.01471568e-02, 9.56035459e-01, 5.21833094e-03],\n",
       "       [7.09364403e-03, 9.12756393e-01, 2.11639547e-02],\n",
       "       [9.30653402e-01, 8.39055514e-02, 2.61730217e-04]])"
      ]
     },
     "execution_count": 1008,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_pred_t  = predict(X_train ,weights,bias)\n",
    "Y_test_pred_t  = predict(X_test ,weights,bias)\n",
    "\n",
    "Y_train_pred_t[19:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "id": "24873599-ee3b-4d6c-99aa-c5ea312d3d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pred =  InverseTransform(Y_train_pred_t ,index_to_label)\n",
    "Y_test_pred = InverseTransform(Y_test_pred_t ,index_to_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "id": "4ba7454e-58f2-4158-9cbe-03ada3997c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38  0  0]\n",
      " [ 0 34  3]\n",
      " [ 0  1 36]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        38\n",
      "           1       0.97      0.92      0.94        37\n",
      "           2       0.92      0.97      0.95        37\n",
      "\n",
      "    accuracy                           0.96       112\n",
      "   macro avg       0.96      0.96      0.96       112\n",
      "weighted avg       0.97      0.96      0.96       112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "print(confusion_matrix(Y_train , Y_train_pred))\n",
    "print(classification_report(Y_train, Y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "id": "3ca93f02-eada-4768-9a2f-eaf5bdbbdb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        38\n",
      "   macro avg       1.00      1.00      1.00        38\n",
      "weighted avg       1.00      1.00      1.00        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(Y_test , Y_test_pred))\n",
    "print(classification_report(Y_test , Y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539afe5a-5006-40ba-877a-ebfb50bb3b9d",
   "metadata": {},
   "source": [
    "## Neural Network Classifier Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "661d2eb5-92b7-4b1e-8a77-297d9b00203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHot_Encoder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self,Y):\n",
    "        uniq = np.unique(Y)\n",
    "        self.n = len(uniq)\n",
    "        \n",
    "        self.label_to_index = {}\n",
    "        self.index_to_label = {}\n",
    "        \n",
    "        index = 0 \n",
    "        while index < self.n:\n",
    "            label = uniq[index]\n",
    "            \n",
    "            self.label_to_index[label] = index\n",
    "            self.index_to_label[index ] = label\n",
    "            index += 1\n",
    "            \n",
    "    def transform(self,Y):\n",
    "        Arr = []\n",
    "        for label in  Y:\n",
    "            vector = np.zeros(self.n)\n",
    "            index = self.label_to_index[label]\n",
    "            vector[index] = 1\n",
    "            Arr.append(vector)\n",
    "            \n",
    "        return np.array(Arr)\n",
    "        \n",
    "    def fit_transform(self,Y):\n",
    "        self.fit(Y) \n",
    "        Y_ = self.transform(Y)\n",
    "        return Y_\n",
    "    \n",
    "        \n",
    "    def inverse_transform(self,Y):\n",
    "        Arr = []    \n",
    "        for vector in  Y:\n",
    "            index = np.argmax(vector)\n",
    "            label = self.index_to_label[index]\n",
    "            Arr.append(label)\n",
    "            \n",
    "        return np.array(Arr) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b088d93-9903-405a-8e15-0542f7b3eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss,confusion_matrix,classification_report\n",
    "\n",
    "class NNClassifier:\n",
    "    def __init__(self,num_hidden=[2,3] , learning_rate = 0.01 , n_iters=1000 ,random_state=10):\n",
    "        self.num_hidden = num_hidden\n",
    "        self.n_iters = n_iters\n",
    "        self.random_state = random_state\n",
    "        self.encoder = OneHot_Encoder()\n",
    "        n_layer = len(num_hidden)+1\n",
    "        lr = learning_rate \n",
    "        if(len(np.shape(learning_rate))==0):\n",
    "            lr  = np.full(n_layer , learning_rate)\n",
    "        elif (len(learning_rate)==n_layer):\n",
    "            lr = learning_rate\n",
    "        else:\n",
    "            print(\"Error with Learning Rate !!\")\n",
    "        self.lr = lr \n",
    "            \n",
    "  ##...Activation Function............................................................................  \n",
    "    def _sig(self,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "    def _derivativeSig(self,z):\n",
    "        return self._sig(z)*(1-self._sig(z))\n",
    "\n",
    "    ## -----Fit-----------------------------------------------------------------------------------\n",
    "    \n",
    "    def _TakeRandomWeightsBias(self,n_feature,n_output):\n",
    "        \n",
    "        np.random.seed(self.random_state)\n",
    "        \n",
    "        num_layers = self.num_hidden + [n_output]\n",
    "        \n",
    "        HLW = np.random.random((n_feature,num_layers[0]))\n",
    "        HLB = np.random.random((1,self.num_hidden[0]))\n",
    "        List1 =[HLW]\n",
    "        List2 =[HLB]\n",
    "        \n",
    "        n = len(num_layers)\n",
    "        j=1\n",
    "        while(j<n):\n",
    "            HLW = np.random.random((num_layers[j-1],num_layers[j]))\n",
    "            HLB = np.random.random((1,num_layers[j]))\n",
    "            List1.append(HLW)\n",
    "            List2.append(HLB)\n",
    "            j+=1\n",
    "\n",
    "        self.weights = List1\n",
    "        self.bias = List2\n",
    "        return\n",
    "    \n",
    "    def _farwardProagation(self):\n",
    "        LayerOutputs = [self._X]\n",
    "        LayerInputs = [self._X]\n",
    "        \n",
    "        j=0\n",
    "        n = len(self.weights)\n",
    "        while j<n:\n",
    "            input = np.dot( LayerOutputs[-1] , self.weights[j] ) + self.bias[j]\n",
    "            output = self._sig(input)\n",
    "            LayerOutputs.append(output)\n",
    "            LayerInputs.append(input)\n",
    "            j+=1\n",
    "\n",
    "        self._outputs = LayerOutputs\n",
    "        self._inputs = LayerInputs\n",
    "        return \n",
    "\n",
    "    def _backwardPropagation(self):\n",
    "        outputs =  self._outputs\n",
    "        inputs =  self._inputs\n",
    "        \n",
    "        first_term = outputs[-1] - self._Y # First Term of Output Layer\n",
    "        \n",
    "        j = len(self.weights)-1\n",
    "        \n",
    "        while j>=0:\n",
    "            \n",
    "            second_term = self._derivativeSig( inputs[j+1] )\n",
    "            first_two = first_term * second_term\n",
    "            \n",
    "            changes_HiddenL_weights = np.dot(outputs[j].T ,first_two)\n",
    "            changes_HiddenL_bias = np.sum(first_two ,axis = 0 ,keepdims = True) ## will give vector rather than single number \n",
    "\n",
    "            print(self.lr[j] * changes_HiddenL_weights)\n",
    "            print(self.lr[j] * changes_HiddenL_bias)\n",
    "            print()\n",
    "            \n",
    "            self.weights[j] -= self.lr[j] * changes_HiddenL_weights\n",
    "            self.bias[j] -= self.lr[j] * changes_HiddenL_bias\n",
    "    \n",
    "            if(j>0):   ## as j==0 it will be useless to find first term ..\n",
    "                \n",
    "                first_term = np.dot( first_two ,self.weights[j].T ) \n",
    "                ## just find first_Term for previpus layer  from first_two of this layer\n",
    "            j-=1\n",
    "            \n",
    "        return \n",
    "        \n",
    "    def fit(self,X,Y):\n",
    "        Y_t = self.encoder.fit_transform(Y)\n",
    "\n",
    "        n_feature = X.shape[1]  \n",
    "        n_output = Y_t.shape[1]\n",
    "        self._TakeRandomWeightsBias(n_feature,n_output) \n",
    "\n",
    "        self._X = X\n",
    "        self._Y = Y_t\n",
    "        for iter in range(self.n_iters ):\n",
    "            self._farwardProagation()\n",
    "            self._backwardPropagation()\n",
    "            # print(iter)\n",
    "            print(iter,1000*log_loss(Y,self.predictProb(X)))\n",
    "    \n",
    "        delattr(self,'_X')     ## As All now useless . of No use \n",
    "        delattr(self,'_Y')\n",
    "        delattr(self,'_outputs')     ## As All now useless . of No use \n",
    "        delattr(self,'_inputs')\n",
    "        return \n",
    "    \n",
    "    ## -----Predict-----------------------------------------------------------------------------------\n",
    "    def predict(self,X):\n",
    "        Y_pred_t = self.predictProb(X)\n",
    "        Y_pred = self.encoder.inverse_transform(Y_pred_t)\n",
    "        return Y_pred\n",
    "        \n",
    "    def predictProb(self,X):\n",
    "        output = X\n",
    "        for w,b in zip( self.weights , self.bias):\n",
    "            input = np.dot(output,w) +b\n",
    "            output = self._sig(input)\n",
    "        Sum = np.sum(output,axis=1,keepdims=True)\n",
    "        output/=Sum\n",
    "        return output\n",
    "## ----Score ----------------------------------------------------------------------------------\n",
    "    def metric(self,X,Y):\n",
    "        Y_pred = self.predict(X)\n",
    "        \n",
    "        print(confusion_matrix(Y, Y_pred))\n",
    "        print(classification_report(Y, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03ba153a-85d2-4541-8f15-f5fcebb8d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa4d8d1f-81a6-4c43-ad65-3f3b9bf3e65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 4)\n",
      "(38, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train ,X_test , Y_train,Y_test = train_test_split(X ,Y ,stratify=Y ,random_state = 42,shuffle=True) \n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40f4d304-ed96-45c5-9948-f2571b5964c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04744844 0.04533994 0.03019006]\n",
      " [0.04425067 0.04223066 0.02810578]\n",
      " [0.04353463 0.04153103 0.02763627]]\n",
      "[[0.04920896 0.04709552 0.03137764]]\n",
      "\n",
      "[[0.00145695 0.00407753 0.00739389]\n",
      " [0.00126539 0.00349425 0.00657159]\n",
      " [0.00145685 0.00407561 0.00739508]\n",
      " [0.00145817 0.00408133 0.00739882]]\n",
      "[[0.00145828 0.0040818  0.00739913]]\n",
      "\n",
      "[[1.72798962e-05 3.53888630e-03 3.27821042e-05 2.94003180e-06]\n",
      " [9.71018736e-06 1.94951817e-03 2.18879341e-05 1.96246101e-06]\n",
      " [9.23500869e-06 1.99509106e-03 1.07184823e-05 9.28810876e-07]\n",
      " [2.60360612e-06 5.82303486e-04 1.93165756e-06 1.55984492e-07]]\n",
      "[[3.22034302e-06 6.31258424e-04 6.61720244e-06 6.02669395e-07]]\n",
      "\n",
      "0 1101.375434566418\n",
      "[[0.04920334 0.04796378 0.03258253]\n",
      " [0.04580829 0.04459251 0.03027673]\n",
      " [0.04496867 0.04375559 0.02970391]]\n",
      "[[0.05104088 0.04983897 0.03387713]]\n",
      "\n",
      "[[0.00135113 0.00389921 0.00743689]\n",
      " [0.0011645  0.00330819 0.0065917 ]\n",
      " [0.00135104 0.00389707 0.0074383 ]\n",
      " [0.0013523  0.00390299 0.00744179]]\n",
      "[[0.0013524  0.00390348 0.00744208]]\n",
      "\n",
      "[[1.67154259e-05 3.52930667e-03 3.20321457e-05 2.92063928e-06]\n",
      " [9.35297739e-06 1.94440852e-03 2.13769575e-05 1.95015912e-06]\n",
      " [8.99221929e-06 1.98610492e-03 1.04898694e-05 9.21457495e-07]\n",
      " [2.53887871e-06 5.78259526e-04 1.89408229e-06 1.54407171e-07]]\n",
      "[[3.11119939e-06 6.29890106e-04 6.46466772e-06 5.98777904e-07]]\n",
      "\n",
      "1 1102.4588868199642\n",
      "[[0.04987909 0.04995026 0.03519314]\n",
      " [0.04636043 0.04635564 0.03264292]\n",
      " [0.04540202 0.04537383 0.03194617]]\n",
      "[[0.05175155 0.05192133 0.03660502]]\n",
      "\n",
      "[[0.00121342 0.00362655 0.00732449]\n",
      " [0.00103561 0.00303851 0.0064742 ]\n",
      " [0.00121332 0.00362417 0.00732613]\n",
      " [0.00121451 0.00363026 0.00732925]]\n",
      "[[0.0012146  0.00363077 0.00732951]]\n",
      "\n",
      "[[1.58490428e-05 3.44482874e-03 3.07388126e-05 2.86226815e-06]\n",
      " [8.82408403e-06 1.89997910e-03 2.05022801e-05 1.91197531e-06]\n",
      " [8.58614544e-06 1.92942449e-03 1.00844089e-05 9.01509389e-07]\n",
      " [2.42610820e-06 5.59062561e-04 1.82463362e-06 1.50631748e-07]]\n",
      "[[2.94615315e-06 6.15713047e-04 6.20243915e-06 5.86918049e-07]]\n",
      "\n",
      "2 1103.921717934415\n",
      "[[0.04915943 0.05090696 0.037997  ]\n",
      " [0.04562132 0.04716213 0.03518217]\n",
      " [0.04456418 0.04604136 0.03433982]]\n",
      "[[0.05101114 0.05293342 0.03953529]]\n",
      "\n",
      "[[0.00105417 0.00326953 0.00704678]\n",
      " [0.00088821 0.00269598 0.00621168]\n",
      " [0.00105407 0.00326691 0.00704865]\n",
      " [0.00105518 0.00327313 0.00705129]]\n",
      "[[0.00105527 0.00327364 0.0070515 ]]\n",
      "\n",
      "[[1.47277998e-05 3.28544884e-03 2.89576862e-05 2.76696150e-06]\n",
      " [8.15229234e-06 1.81645996e-03 1.93010201e-05 1.84918749e-06]\n",
      " [8.03736589e-06 1.82433696e-03 9.52020342e-06 8.69761570e-07]\n",
      " [2.27037831e-06 5.24337322e-04 1.72654247e-06 1.44830460e-07]]\n",
      "[[2.73433166e-06 5.88810062e-04 5.84174783e-06 5.67498758e-07]]\n",
      "\n",
      "3 1105.7876329514102\n",
      "[[0.04693729 0.05048494 0.04093335]\n",
      " [0.04349994 0.04669659 0.03783979]\n",
      " [0.04237925 0.04546082 0.03683114]]\n",
      "[[0.04870728 0.0525109  0.0426045 ]]\n",
      "\n",
      "[[0.00089006 0.00285229 0.00662483]\n",
      " [0.00073723 0.00230395 0.00582383]\n",
      " [0.00088995 0.00284946 0.00662693]\n",
      " [0.00089097 0.00285572 0.00662901]]\n",
      "[[0.00089106 0.00285624 0.00662918]]\n",
      "\n",
      "[[1.34528613e-05 3.06532424e-03 2.68320901e-05 2.64212585e-06]\n",
      " [7.39691708e-06 1.70136472e-03 1.78687806e-05 1.76660836e-06]\n",
      " [7.39621474e-06 1.67941561e-03 8.84437815e-06 8.28791616e-07]\n",
      " [2.08560415e-06 4.76683319e-04 1.60839217e-06 1.37499931e-07]]\n",
      "[[2.49484196e-06 5.51662688e-04 5.41150193e-06 5.42021758e-07]]\n",
      "\n",
      "4 1107.984733119326\n",
      "[[0.04340407 0.04852356 0.04388693]\n",
      " [0.04018004 0.04481828 0.0405114 ]\n",
      " [0.03903995 0.04350905 0.03931965]]\n",
      "[[0.04503827 0.05048527 0.0456925 ]]\n",
      "\n",
      "[[0.00073857 0.00240627 0.00610723]\n",
      " [0.00059815 0.00189192 0.00535404]\n",
      " [0.00073846 0.00240325 0.00610953]\n",
      " [0.00073941 0.0024095  0.00611104]]\n",
      "[[0.00073949 0.00241002 0.00611116]]\n",
      "\n",
      "[[1.21487538e-05 2.80877457e-03 2.45474028e-05 2.49773965e-06]\n",
      " [6.62981643e-06 1.56727154e-03 1.63292866e-05 1.67076754e-06]\n",
      " [6.72782617e-06 1.51042165e-03 8.11792033e-06 7.81998791e-07]\n",
      " [1.89057782e-06 4.21117069e-04 1.48133920e-06 1.29279403e-07]]\n",
      "[[2.25092083e-06 5.08416022e-04 4.94907826e-06 5.12514671e-07]]\n",
      "\n",
      "5 1110.3099234731053\n",
      "[[0.03900223 0.04516086 0.04666713]\n",
      " [0.03607393 0.04166072 0.04302361]\n",
      " [0.03495778 0.04032966 0.04163959]]\n",
      "[[0.04046399 0.04699903 0.04860072]]\n",
      "\n",
      "[[0.00061062 0.00195963 0.00554957]\n",
      " [0.00048065 0.00148593 0.00485148]\n",
      " [0.00061051 0.00195648 0.00555202]\n",
      " [0.00061141 0.00196263 0.00555299]]\n",
      "[[0.00061148 0.00196314 0.00555307]]\n",
      "\n",
      "[[1.09103138e-05 2.53944181e-03 2.22464171e-05 2.34132533e-06]\n",
      " [5.90476294e-06 1.42608109e-03 1.47779903e-05 1.56661162e-06]\n",
      " [6.08499915e-06 1.33372056e-03 7.38775147e-06 7.31914011e-07]\n",
      " [1.70140133e-06 3.63076132e-04 1.35400721e-06 1.20639693e-07]]\n",
      "[[2.01996708e-06 4.62986656e-04 4.48325897e-06 4.80507557e-07]]\n",
      "\n",
      "6 1112.4400841343256\n",
      "[[0.03425929 0.04081358 0.04899388]\n",
      " [0.03166874 0.03761135 0.04512092]\n",
      " [0.03061057 0.03630866 0.04354765]]\n",
      "[[0.03553326 0.04248532 0.05103742]]\n",
      "\n",
      "[[0.00050626 0.00152974 0.00498971]\n",
      " [0.00038497 0.00110187 0.00435   ]\n",
      " [0.00050615 0.00152652 0.00499229]\n",
      " [0.000507   0.00153249 0.00499276]]\n",
      "[[0.00050707 0.00153299 0.0049928 ]]\n",
      "\n",
      "[[9.76194001e-06 2.26970042e-03 1.99589070e-05 2.17390569e-06]\n",
      " [5.23396173e-06 1.28348667e-03 1.32349320e-05 1.45482787e-06]\n",
      " [5.48682637e-06 1.15952485e-03 6.66379353e-06 6.78876683e-07]\n",
      " [1.52531422e-06 3.06283035e-04 1.22835067e-06 1.11647699e-07]]\n",
      "[[1.80594326e-06 4.17252899e-04 4.02001581e-06 4.46207640e-07]]\n",
      "\n",
      "7 1114.0050640753338\n",
      "[[0.02961892 0.03602142 0.05050897]\n",
      " [0.02737143 0.03316744 0.04647615]\n",
      " [0.02639264 0.03193225 0.04473382]]\n",
      "[[0.03070789 0.03750569 0.05262934]]\n",
      "\n",
      "[[0.00041684 0.0011237  0.0044377 ]\n",
      " [0.0003039  0.000746   0.00385957]\n",
      " [0.00041673 0.00112047 0.00444039]\n",
      " [0.00041753 0.00112618 0.00444039]]\n",
      "[[0.0004176  0.00112666 0.00444039]]\n",
      "\n",
      "[[8.66091828e-06 1.99864295e-03 1.76034256e-05 1.99052370e-06]\n",
      " [4.59014150e-06 1.13806261e-03 1.16457148e-05 1.33216657e-06]\n",
      " [4.91934417e-06 9.90112040e-04 5.91984783e-06 6.21236547e-07]\n",
      " [1.36049870e-06 2.52064086e-04 1.09985593e-06 1.02008792e-07]]\n",
      "[[1.60010896e-06 3.70765748e-04 3.54286000e-06 4.08603676e-07]]\n",
      "\n",
      "8 1114.7025361746607\n",
      "CPU times: total: 78.1 ms\n",
      "Wall time: 64.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Without Any Hidden Layer \n",
    "# lr = 0.005\n",
    "# num_hidden=[]\n",
    "# iterations=10000\n",
    "\n",
    "# # With Hidden Layers \n",
    "# lr = 0.005\n",
    "# num_hidden=[2,3]\n",
    "# iterations=10000\n",
    "\n",
    "# With Hidden Layers \n",
    "lr = 0.006\n",
    "num_hidden=[4,3]\n",
    "iterations=9\n",
    "\n",
    "## Multiple Hidden Layers\n",
    "# lr = np.array([1,1,1,10])*0.01\n",
    "# num_hidden=[2,3,7]\n",
    "# iterations=5000\n",
    "\n",
    "clf = NNClassifier(num_hidden=num_hidden,learning_rate=lr,n_iters=iterations)\n",
    "clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "859b0297-20da-4258-b384-c27ade1883e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 1, 1, 2, 0, 2, 0, 2, 0, 2, 1, 0, 0, 1, 2, 0, 0, 1, 1, 1,\n",
       "       0, 1, 2, 0, 2, 1, 2, 0, 0, 1, 0, 2, 0, 0, 1, 0, 1, 0, 0, 1, 2, 2,\n",
       "       0, 2, 1, 0, 2, 0, 2, 2, 0, 1, 2, 2, 1, 2, 0, 2, 1, 2, 2, 2, 0, 1,\n",
       "       0, 2, 1, 2, 1, 2, 2, 0, 2, 1, 0, 2, 0, 2, 1, 1, 0, 2, 2, 0, 0, 2,\n",
       "       2, 1, 2, 0, 2, 1, 2, 2, 0, 1, 1, 1, 1, 1, 0, 2, 2, 1, 0, 0, 0, 0,\n",
       "       1, 0])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_pred = clf.predict(X_train)\n",
    "Y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "01d9e6ee-d620-4b62-9213-9146cd7fc69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38  0  0]\n",
      " [ 0 33  4]\n",
      " [ 0  0 37]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        38\n",
      "           1       1.00      0.89      0.94        37\n",
      "           2       0.90      1.00      0.95        37\n",
      "\n",
      "    accuracy                           0.96       112\n",
      "   macro avg       0.97      0.96      0.96       112\n",
      "weighted avg       0.97      0.96      0.96       112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.metric(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c92f6126-a99b-431a-abfe-fd4e26a04bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12  0  0]\n",
      " [ 0 11  2]\n",
      " [ 0  0 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       1.00      0.85      0.92        13\n",
      "           2       0.87      1.00      0.93        13\n",
      "\n",
      "    accuracy                           0.95        38\n",
      "   macro avg       0.96      0.95      0.95        38\n",
      "weighted avg       0.95      0.95      0.95        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.metric(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc70ddb4-14af-44ed-9d90-aa056ef2bfd5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Practice  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "id": "83336dea-c159-42ac-8fbf-3dd1addf1e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1044,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.shape(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb785f44-4a92-49d2-9459-d63a64dfc68e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e404f25-e8ce-4f2b-8be2-3e7fe02cf9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b055344-4681-4bfe-be8f-6ab59d3130dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfa19a8-3df9-4cd0-b31e-46f5264606c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40ebe6-7292-4164-b4e7-8b85bfe1ca74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
